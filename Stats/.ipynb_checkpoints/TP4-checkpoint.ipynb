{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse de Données - Développeur\n",
    "## TP4 : Classification Supervisée \n",
    "## 0) Tableau de Données de l'Exercice 1 du TD4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    X1  X2  Y\n",
      "0    0   2  1\n",
      "1    2   0  1\n",
      "2    2   6  1\n",
      "3    4   4  1\n",
      "4    2   4  2\n",
      "5    6   7  2\n",
      "6   10   4  2\n",
      "7    4   0  3\n",
      "8    4   2  3\n",
      "9    6   0  3\n",
      "10   6   2  3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_excel(\"TD4-Ex1.xlsx\")\n",
    "print(df)\n",
    "X = df.values[:,:2]\n",
    "Y = df.values[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Nearest Prototype Classifier\n",
    "### 1.1) Disposez-vous de toutes les fonctions utiles au codage de cette règle de classification supervisée ? Importez-les. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(x,y,dname='euclidean'):\n",
    "    d = np.abs(x-y)\n",
    "    if dname == 'manhattan' or dname == 'cityblock':\n",
    "        return sum(d)\n",
    "    elif dname in ['chebyshev', 'chebychev']:\n",
    "        return max(d)\n",
    "    elif dname in [\"cosinus\", \"cosine\", \"cos\"]:\n",
    "        return 1 - x.dot(y)/np.sqrt(x.dot(x)*y.dot(y))\n",
    "    else:\n",
    "        return np.sqrt(sum(d**2))\n",
    "\n",
    "def clustering(data,centroids,group_names=None,dname='eulidean'):\n",
    "    \"\"\"Group data by clusters, given some centroids.\"\"\"\n",
    "    # default to 1, 2, 3, …, n\n",
    "    if group_names is None:\n",
    "        group_names = [i+1 for i in range(len(centroids))]\n",
    "\n",
    "    distances = []\n",
    "    groups = []\n",
    "    for d in data:\n",
    "        t = []\n",
    "        for c in centroids:\n",
    "            # On détermine la distance au centroid\n",
    "            t.append(dist(d,c,dname))\n",
    "        # On détermine le groupe pour cet individu\n",
    "        group = t.index(min(t))\n",
    "        groups.append(group_names[group])\n",
    "        distances.append(t)\n",
    "    # Argmin peut faire des trucs cools, genre retourner l'index de la valeur min\n",
    "    return np.array(distances), groups\n",
    "\n",
    "def prototyping(data, partition):\n",
    "    clust_id = np.unique(partition)\n",
    "    centroids = []\n",
    "    for id in clust_id:\n",
    "        elms = data[partition == id]\n",
    "        centroids.append(elms.mean(axis=0))\n",
    "    return np.array(centroids)\n",
    "\n",
    "def partitionmatrix(data_clusters):\n",
    "    data_clusters = np.array(data_clusters)\n",
    "    n = data_clusters.shape[0]\n",
    "    clust_id = np.unique(data_clusters)\n",
    "    clust_nb = len(clust_id)\n",
    "    data_partition = np.zeros((n,clust_nb))\n",
    "    for k in range(n):\n",
    "        data_partition[k,clust_id==data_clusters[k]] = 1\n",
    "    return data_partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) Ecrivez une fonction *NPClassifier* permettant de prédire, pour tous les points d'un tableau *test*, leur prédiction *test_labels* avec la règle du Plus Proche Prototype d'un ensemble d'apprentissage (tableau *learn*, indicatice de groupe *learn_labels*) au sens d'une distance *dname* (euclidienne par défaut). \n",
    "### Vous testerez sur un tableau *Xtest* comprenant les points $x$, $y$ et $z$ du TD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 2, 2, 3, 3, 3, 3]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest = np.array([[3,2],[10,2],[5,3]])\n",
    "def NPClassifier(test, learn, learn_labels,dname='euclidean'):\n",
    "    prototypes = prototyping(learn,learn_labels)\n",
    "    test_labels = clustering(test,prototypes,None,dname)[1]\n",
    "    return test_labels\n",
    "\n",
    "Ytest = NPClassifier(Xtest,X,Y)\n",
    "Ypred = NPClassifier(X,X,Y)\n",
    "print(Ytest)\n",
    "Ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3) Reclassez les données d'apprentissage. Tous les points sont-ils bien reclassés ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4) Trouvez un moyen \n",
    "#### 1.4.1) simple de calculer la matrice de confusion (table de contingence croisant les *learn_labels* et les *pred_labels* d'un ensemble d'apprentissage (tableau *learn*, indicatice de groupe *learn_labels*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 0., 0.],\n",
       "       [1., 2., 0.],\n",
       "       [0., 0., 4.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_part = partitionmatrix(Y)\n",
    "test_part = partitionmatrix(Ypred)\n",
    "confusion =  np.dot(learn_part.T,test_part)\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2) en déduire la *Classification Accuracy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4. 0. 0.]\n",
      " [1. 2. 0.]\n",
      " [0. 0. 4.]]\n",
      "0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "def classif_acc(actual,predicted):\n",
    "    learn_part = partitionmatrix(actual)\n",
    "    pred_part = partitionmatrix(predicted)\n",
    "    confusion =  np.dot(learn_part.T,pred_part)\n",
    "    accuracy = np.diag(confusion).sum() / confusion.sum()\n",
    "    return accuracy, confusion\n",
    "\n",
    "CA, conf = classif_acc(Y,Ypred)\n",
    "\n",
    "print(conf)\n",
    "print(CA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6428571428571429, array([[4., 2., 0.],\n",
       "        [0., 2., 1.],\n",
       "        [1., 1., 3.]]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [3,1,1,2,3,3,2,2,3,3,1,1,1,1]\n",
    "p = [3,1,2,2,1,3,2,3,2,3,1,1,1,2]\n",
    "classif_acc(a,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Nearest Neighbors Classifier\n",
    "### 2.1) Ecrivez une fonction *NNClassifier* permettant de prédire, pour tous les points d'un tableau *test*, leur prédiction *test_labels* avec la règle des K Plus Proches Voisins d'un ensemble d'apprentissage (tableau *learn*, indicatice de groupe *learn_labels*) au sens d'une distance *dname* (euclidienne par défaut). \n",
    "### Pour déterminer le groupe majoritairement représenté parmi les *K* voisins, utilisez la fonction *histogram* de *numpy*.\n",
    "### Vous testerez sur le tableau *Xtest* comprenant les points $x$, $y$ et $z$ du TD, avec *K=5*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3,  8, 10,  4,  7]), array([1, 3, 3, 2, 3]), array([[2., 1., 2.],\n",
       "        [1., 1., 3.],\n",
       "        [1., 1., 3.]]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest = np.array([[3,2],[10,2],[5,3]])\n",
    "def NNClassifier(test, learn, learn_labels, K=1, dname='euclidean'):\n",
    "    test_nb = test.shape[0]\n",
    "    learn_nb = learn.shape[0]\n",
    "    labels_id = np.unique(learn_labels)\n",
    "    labels_nb = len(labels_id)\n",
    "    dist2learn = np.zeros((test_nb,learn_nb))\n",
    "    votes = np.zeros((test_nb,labels_nb))\n",
    "    \n",
    "    for i in range(test_nb):\n",
    "        for k in range(learn_nb):\n",
    "            dist2learn[i,k] = dist(test[i,:],learn[k,:],dname)\n",
    "            # indexes des K-NN\n",
    "            knn_index = np.argsort(dist2learn[i,:])[:K]\n",
    "            knn_labels = learn_labels[knn_index]\n",
    "            for j in range(labels_nb):\n",
    "                votes[i,j] = len(np.argwhere(knn_labels == labels_id[j]))\n",
    "   \n",
    "    return knn_index, knn_labels, votes\n",
    "\n",
    "NNClassifier(Xtest,X,Y,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Peut-on reclasser les données d'apprentisage ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9090909090909091"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xpred = NNClassifier(X,X,Y,5)[1]\n",
    "CA, conf = classif_acc(Y,Ypred)\n",
    "CA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Données de *Machine Learning*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_excel(\"../Data/MLlogiciel.xlsx\")\n",
    "T = df.values[5:,1:4]; \n",
    "T_labels = df.values[5:,:1]\n",
    "Tbar = df.values[:5,1:4]; \n",
    "Tbar_labels= df.values[:5,:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) En utilisant *NPCLassifier*, réalisez les prédictions pour les données du tableau *Tbar* à partir de l'ensemble d'apprentissage (*T,T_labels*) et calculez la *CA*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) Recommencez en échangeant les deux ensembles (*T,T_labels*) et (*Tbar,Tbar_labels*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3) Reprenez les questions 3.1 et 3.2 avec *NNClassifier. Vous pourrez tester plusieurs valeurs de *K* et de *dname*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
